{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2297ff4e",
   "metadata": {},
   "source": [
    "# NLP Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                                            2211CS020050\n",
    "                                                                                                            AIML -Alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507a66a",
   "metadata": {},
   "source": [
    "# 1.Correct the Search Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43bc6a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bed5c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eefc9183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: Whre is the nerest restrunt near by?\n",
      "Corrected Query: nearest restraint near ?\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "def correct_query(query):\n",
    "    blob = TextBlob(query)\n",
    "    corrected_query = str(blob.correct())\n",
    "    tokens = word_tokenize(corrected_query.lower()) \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    refined_query = ' '.join(filtered_tokens)\n",
    "    return refined_query\n",
    "original_query = \"Whre is the nerest restrunt near by?\"\n",
    "corrected_query = correct_query(original_query)\n",
    "print(\"Original Query:\", original_query)\n",
    "print(\"Corrected Query:\", corrected_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09032b32",
   "metadata": {},
   "source": [
    "# 2 Deterministic Url and HashTag Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c0810ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Check out https://example.com/path/to/page?param1=value1&param2=value2 and the trending #HelloWorld and #NaturalLanguageProcessing hashtags!\n",
      "\n",
      "Segmented URLs:\n",
      "['Domain: example.com, Path: / path / to / page, Query: param1=value1 & param2=value2']\n",
      "\n",
      "Segmented Hashtags:\n",
      "['Hello World', 'Natural Language Processing']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "def segment_url(text):\n",
    "    url_pattern = re.compile(r'https?://[^\\s]+')\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    segmented_urls = []\n",
    "    for url in urls:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc\n",
    "        path = parsed.path.replace('/', ' / ').strip()\n",
    "        query = parsed.query.replace('&', ' & ').strip()\n",
    "        segments = f\"Domain: {domain}, Path: {path}, Query: {query}\" if query else f\"Domain: {domain}, Path: {path}\"\n",
    "        segmented_urls.append(segments)\n",
    "    return segmented_urls\n",
    "def segment_hashtag(hashtag):\n",
    "    camel_case_split = re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?=[A-Z]|$)', hashtag)\n",
    "    return ' '.join(camel_case_split).strip()\n",
    "def process_text(text):\n",
    "    url_pattern = re.compile(r'https?://[^\\s]+')\n",
    "    hashtag_pattern = re.compile(r'#\\w+')\n",
    "    urls = url_pattern.findall(text)\n",
    "    hashtags = hashtag_pattern.findall(text)\n",
    "    segmented_urls = segment_url(text)\n",
    "    segmented_hashtags = [segment_hashtag(ht[1:]) for ht in hashtags]  # Remove '#' from hashtags\n",
    "\n",
    "    return {\n",
    "        \"Original Text\": text,\n",
    "        \"Segmented URLs\": segmented_urls,\n",
    "        \"Segmented Hashtags\": segmented_hashtags\n",
    "    }\n",
    "input_text = \"Check out https://example.com/path/to/page?param1=value1&param2=value2 and the trending #HelloWorld and #NaturalLanguageProcessing hashtags!\"\n",
    "result = process_text(input_text)\n",
    "print(\"Original Text:\")\n",
    "print(result[\"Original Text\"])\n",
    "print(\"\\nSegmented URLs:\")\n",
    "print(result[\"Segmented URLs\"])\n",
    "print(\"\\nSegmented Hashtags:\")\n",
    "print(result[\"Segmented Hashtags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d135ec",
   "metadata": {},
   "source": [
    "# 3.Disambiguation: Mouse vs Mouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb5c1536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Disambiguation Result:\n",
      "Sentence: I connected the mouse to the computer.\n",
      "Sense: shiner.n.01\n",
      "Definition: a swollen bruise caused by a blow to the eye\n",
      "Examples: []\n",
      "\n",
      "Disambiguation Result:\n",
      "Sentence: The mouse ran across the kitchen floor.\n",
      "Sense: shiner.n.01\n",
      "Definition: a swollen bruise caused by a blow to the eye\n",
      "Examples: []\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "def disambiguate_mouse(sentence):\n",
    "    \"\"\"\n",
    "    Disambiguates the word 'mouse' using WordNet and context.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    sense = lesk(tokens, \"mouse\", \"n\") \n",
    "\n",
    "    if sense:\n",
    "        return {\n",
    "            \"Sentence\": sentence,\n",
    "            \"Word\": \"mouse\",\n",
    "            \"Sense\": sense.name(),  \n",
    "            \"Definition\": sense.definition(),  \n",
    "            \"Examples\": sense.examples() \n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"Sentence\": sentence,\n",
    "            \"Word\": \"mouse\",\n",
    "            \"Sense\": None,\n",
    "            \"Message\": \"No suitable sense found.\"\n",
    "        }\n",
    "sentences = [\n",
    "    \"I connected the mouse to the computer.\",\n",
    "    \"The mouse ran across the kitchen floor.\"\n",
    "]\n",
    "for sentence in sentences:\n",
    "    result = disambiguate_mouse(sentence)\n",
    "    print(\"\\nDisambiguation Result:\")\n",
    "    print(f\"Sentence: {result['Sentence']}\")\n",
    "    print(f\"Sense: {result['Sense']}\")\n",
    "    print(f\"Definition: {result['Definition']}\")\n",
    "    print(f\"Examples: {result['Examples']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe87d5a",
   "metadata": {},
   "source": [
    "# 4.Language Detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef1b04cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langdetect in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "454fa21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Language Detection Result:\n",
      "Text: Bonjour tout le monde\n",
      "Detected Language: fr\n",
      "Language Probabilities: [fr:0.9999952854881803]\n",
      "\n",
      "Language Detection Result:\n",
      "Text: Hola, ¿cómo estás?\n",
      "Detected Language: es\n",
      "Language Probabilities: [es:0.9999980348026196]\n",
      "\n",
      "Language Detection Result:\n",
      "Text: Hello, how are you?\n",
      "Detected Language: en\n",
      "Language Probabilities: [en:0.857140367991237, so:0.1428566996647734]\n",
      "\n",
      "Language Detection Result:\n",
      "Text: こんにちは、お元気ですか？\n",
      "Detected Language: ja\n",
      "Language Probabilities: [ja:0.9999993081257395]\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect, detect_langs\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        lang_probs = detect_langs(text)\n",
    "        return {\n",
    "            \"Text\": text,\n",
    "            \"Detected Language\": lang,\n",
    "            \"Language Probabilities\": str(lang_probs)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"Text\": text,\n",
    "            \"Error\": str(e)\n",
    "        }\n",
    "\n",
    "texts = [\n",
    "    \"Bonjour tout le monde\",        # French\n",
    "    \"Hola, ¿cómo estás?\",          # Spanish\n",
    "    \"Hello, how are you?\",         # English\n",
    "    \"こんにちは、お元気ですか？\",   # Japanese\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    result = detect_language(text)\n",
    "    print(\"\\nLanguage Detection Result:\")\n",
    "    print(f\"Text: {result['Text']}\")\n",
    "    print(f\"Detected Language: {result['Detected Language']}\")\n",
    "    print(f\"Language Probabilities: {result['Language Probabilities']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e215227e",
   "metadata": {},
   "source": [
    "# 5.The Missing Apostrophes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0d015e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: dont forget to call Johns sister\n",
      "Corrected: don't forget to call John's sister\n",
      "\n",
      "Original: im going to the doctors office\n",
      "Corrected: I'm going to the doctor's office\n",
      "\n",
      "Original: hes not sure whats happening\n",
      "Corrected: he's not sure what's happening\n",
      "\n",
      "Original: thats the teachers book\n",
      "Corrected: that's the teacher's book\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "CONTRACTIONS = {\n",
    "    \"im\": \"I'm\",\n",
    "    \"dont\": \"don't\",\n",
    "    \"cant\": \"can't\",\n",
    "    \"wont\": \"won't\",\n",
    "    \"ive\": \"I've\",\n",
    "    \"id\": \"I'd\",\n",
    "    \"hes\": \"he's\",\n",
    "    \"shes\": \"she's\",\n",
    "    \"theyre\": \"they're\",\n",
    "    \"youre\": \"you're\",\n",
    "    \"thats\": \"that's\",\n",
    "    \"whos\": \"who's\",\n",
    "    \"whats\": \"what's\",\n",
    "    \"lets\": \"let's\",\n",
    "    \"theres\": \"there's\",\n",
    "    \"heres\": \"here's\"\n",
    "}\n",
    "\n",
    "def correct_apostrophes(text):\n",
    "    \"\"\"\n",
    "    Corrects missing apostrophes in contractions and possessives.\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    corrected_tokens = [CONTRACTIONS.get(word.lower(), word) for word in tokens]\n",
    "    corrected_text = \" \".join(corrected_tokens)\n",
    "    possessive_pattern = re.compile(r\"\\b(\\w+)(s)\\b\")\n",
    "    corrected_text = possessive_pattern.sub(r\"\\1's\", corrected_text)\n",
    "\n",
    "    return corrected_text\n",
    "texts = [\n",
    "    \"dont forget to call Johns sister\",\n",
    "    \"im going to the doctors office\",\n",
    "    \"hes not sure whats happening\",\n",
    "    \"thats the teachers book\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    corrected = correct_apostrophes(text)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Corrected: {corrected}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad7a4a",
   "metadata": {},
   "source": [
    "# 6.Segment the Twitter Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9360fdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: #NaturalLanguageProcessing\n",
      "Segmented: Natural Language Processing\n",
      "\n",
      "Original: #HelloWorld\n",
      "Segmented: Hello World\n",
      "\n",
      "Original: #machinelearning\n",
      "Segmented: machine learning\n",
      "\n",
      "Original: #DeepNeuralNetworks\n",
      "Segmented: Deep Neural Networks\n",
      "\n",
      "Original: #artificialintelligence\n",
      "Segmented: artificial intelligence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "DICTIONARY = set([\n",
    "    \"natural\", \"language\", \"processing\", \"hello\", \"world\", \"artificial\", \n",
    "    \"intelligence\", \"machine\", \"learning\", \"deep\", \"neural\", \"networks\", \"the\"\n",
    "])\n",
    "\n",
    "def camel_case_split(hashtag):\n",
    "    \"\"\"\n",
    "    Splits hashtags written in CamelCase.\n",
    "    \"\"\"\n",
    "    return re.sub(r'([a-z])([A-Z])', r'\\1 \\2', hashtag).split()\n",
    "\n",
    "def dictionary_based_split(hashtag):\n",
    "    \"\"\"\n",
    "    Splits hashtags without CamelCase based on a dictionary of valid words.\n",
    "    \"\"\"\n",
    "    hashtag = hashtag.lower()  \n",
    "    n = len(hashtag)\n",
    "    results = []\n",
    "\n",
    "    def backtrack(index, path):\n",
    "        if index == n:\n",
    "            results.append(\" \".join(path))\n",
    "            return\n",
    "        for end in range(index + 1, n + 1):\n",
    "            word = hashtag[index:end]\n",
    "            if word in DICTIONARY:\n",
    "                backtrack(end, path + [word])\n",
    "\n",
    "    backtrack(0, [])\n",
    "    return results[0] if results else hashtag  # Return first valid segmentation\n",
    "\n",
    "def segment_hashtag(hashtag):\n",
    "    \"\"\"\n",
    "    Main function to segment a hashtag.\n",
    "    \"\"\"\n",
    "    hashtag = hashtag.lstrip('#')  # Remove '#' symbol\n",
    "    if any(c.isupper() for c in hashtag):  # CamelCase detection\n",
    "        return \" \".join(camel_case_split(hashtag))\n",
    "    else:\n",
    "        return dictionary_based_split(hashtag)\n",
    "\n",
    "\n",
    "hashtags = [\n",
    "    \"#NaturalLanguageProcessing\",\n",
    "    \"#HelloWorld\",\n",
    "    \"#machinelearning\",\n",
    "    \"#DeepNeuralNetworks\",\n",
    "    \"#artificialintelligence\"\n",
    "]\n",
    "\n",
    "for ht in hashtags:\n",
    "    segmented = segment_hashtag(ht)\n",
    "    print(f\"Original: {ht}\")\n",
    "    print(f\"Segmented: {segmented}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79139621",
   "metadata": {},
   "source": [
    "# 7. Expand the Acronyms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6816ec10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: AI and NLP are transforming the tech industry.\n",
      "Expanded: Artificial Intelligence and Natural Language Processing are transforming the tech industry.\n",
      "\n",
      "Original: ML models require a GPU for faster computation.\n",
      "Expanded: Machine Learning models require a Graphics Processing Unit for faster computation.\n",
      "\n",
      "Original: HTML and HTTP are fundamental to web development.\n",
      "Expanded: HyperText Markup Language and HyperText Transfer Protocol are fundamental to web development.\n",
      "\n",
      "Original: The IoT is changing how devices communicate.\n",
      "Expanded: The IOT is changing how devices communicate.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "ACRONYM_DICT = {\n",
    "    \"AI\": \"Artificial Intelligence\",\n",
    "    \"NLP\": \"Natural Language Processing\",\n",
    "    \"ML\": \"Machine Learning\",\n",
    "    \"API\": \"Application Programming Interface\",\n",
    "    \"IoT\": \"Internet of Things\",\n",
    "    \"GPU\": \"Graphics Processing Unit\",\n",
    "    \"HTML\": \"HyperText Markup Language\",\n",
    "    \"HTTP\": \"HyperText Transfer Protocol\"\n",
    "}\n",
    "\n",
    "def expand_acronyms(text):\n",
    "    \"\"\"\n",
    "    Expands acronyms in a given text based on a predefined dictionary.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in ACRONYM_DICT.keys()) + r')\\b', re.IGNORECASE)\n",
    "    def replace_acronym(match):\n",
    "        acronym = match.group(0).upper()  # Preserve case\n",
    "        return ACRONYM_DICT.get(acronym, acronym)\n",
    "    expanded_text = pattern.sub(replace_acronym, text)\n",
    "    return expanded_text\n",
    "sentences = [\n",
    "    \"AI and NLP are transforming the tech industry.\",\n",
    "    \"ML models require a GPU for faster computation.\",\n",
    "    \"HTML and HTTP are fundamental to web development.\",\n",
    "    \"The IoT is changing how devices communicate.\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    expanded = expand_acronyms(sentence)\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Expanded: {expanded}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607492d",
   "metadata": {},
   "source": [
    "# 8. Correct the Search Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d107b596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: Whre is the nerest restrunt near by?\n",
      "Corrected Query: nearest restraint near ?\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "def correct_query(query):\n",
    "    blob = TextBlob(query)\n",
    "    corrected_query = str(blob.correct())\n",
    "    tokens = word_tokenize(corrected_query.lower()) \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    refined_query = ' '.join(filtered_tokens)\n",
    "    return refined_query\n",
    "original_query = \"Whre is the nerest restrunt near by?\"\n",
    "corrected_query = correct_query(original_query)\n",
    "print(\"Original Query:\", original_query)\n",
    "print(\"Corrected Query:\", corrected_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5208a3f6",
   "metadata": {},
   "source": [
    "# 9.A Text-Processing Warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c13f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "Natural Language Processing (NLP) is a fascinating field that bridges the gap \n",
      "between human communication and machine understanding. NLP techniques include \n",
      "tokenization, stemming, and removing stopwords. These preprocessing steps are \n",
      "critical for tasks like sentiment analysis, text classification, and more.\n",
      "\n",
      "\n",
      "Tokenized:\n",
      "['Natural', 'Language', 'Processing', 'NLP', 'is', 'a', 'fascinating', 'field', 'that', 'bridges', 'the', 'gap', 'between', 'human', 'communication', 'and', 'machine', 'understanding', 'NLP', 'techniques', 'include', 'tokenization', 'stemming', 'and', 'removing', 'stopwords', 'These', 'preprocessing', 'steps', 'are', 'critical', 'for', 'tasks', 'like', 'sentiment', 'analysis', 'text', 'classification', 'and', 'more']\n",
      "\n",
      "Filtered Tokens:\n",
      "['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'bridges', 'gap', 'human', 'communication', 'machine', 'understanding', 'NLP', 'techniques', 'include', 'tokenization', 'stemming', 'removing', 'stopwords', 'preprocessing', 'steps', 'critical', 'tasks', 'like', 'sentiment', 'analysis', 'text', 'classification']\n",
      "\n",
      "Stemmed Tokens:\n",
      "['natur', 'languag', 'process', 'nlp', 'fascin', 'field', 'bridg', 'gap', 'human', 'commun', 'machin', 'understand', 'nlp', 'techniqu', 'includ', 'token', 'stem', 'remov', 'stopword', 'preprocess', 'step', 'critic', 'task', 'like', 'sentiment', 'analysi', 'text', 'classif']\n",
      "\n",
      "Top Words:\n",
      "[('nlp', 2), ('natur', 1), ('languag', 1), ('process', 1), ('fascin', 1)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ruchi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "def text_processing_warmup(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    word_frequencies = Counter(stemmed_tokens)\n",
    "    top_words = word_frequencies.most_common(5)\n",
    "    \n",
    "    return {\n",
    "        \"Original Text\": text,\n",
    "        \"Tokenized\": tokens,\n",
    "        \"Filtered Tokens\": filtered_tokens,\n",
    "        \"Stemmed Tokens\": stemmed_tokens,\n",
    "        \"Top Words\": top_words\n",
    "    }\n",
    "text = \"\"\"\n",
    "Natural Language Processing (NLP) is a fascinating field that bridges the gap \n",
    "between human communication and machine understanding. NLP techniques include \n",
    "tokenization, stemming, and removing stopwords. These preprocessing steps are \n",
    "critical for tasks like sentiment analysis, text classification, and more.\n",
    "\"\"\"\n",
    "result = text_processing_warmup(text)\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}:\\n{value}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f9f2f",
   "metadata": {},
   "source": [
    "# 10.Who is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d06de84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (4.48.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from transformers) (0.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ruchi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aae6853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f415521d15b4a9a9904df9a718f2828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruchi\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ruchi\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30644\\3184887520.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Load a pre-trained question-answering model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mqa_pipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"question-answering\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Define context and question\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mframework\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[0mmodel_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m         framework, model = infer_framework_load_model(\n\u001b[0m\u001b[0;32m    941\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mmodel_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \"\"\"\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         raise RuntimeError(\n\u001b[0m\u001b[0;32m    241\u001b[0m             \u001b[1;34m\"At least one of TensorFlow 2.0 or PyTorch should be installed. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[1;34m\"To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "context = \"The person at the door is John.\"\n",
    "question = \"Who is it?\"\n",
    "answer = qa_pipeline(question=question, context=context)\n",
    "print(f\"Answer: {answer['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a62d77d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
